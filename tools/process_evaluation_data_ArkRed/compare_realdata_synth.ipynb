{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import os\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from bokeh.plotting import figure, output_file, save\n",
    "from bokeh.io import reset_output\n",
    "import bokeh\n",
    "import properscoring as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================== #\n",
    "# Parameters\n",
    "# ===================================================== #\n",
    "# --- USGS data --- #\n",
    "# Site info file; needs to have columns \"short_name\" and \"site_no\"\n",
    "site_info_csv = '/civil/hydro/ymao/data_assim/data/USGS/Crow_2017/sites_in_ArkRed/site_info_in_ArkRed.csv'\n",
    "# Directory of USGS data\n",
    "usgs_data_dir = '/civil/hydro/ymao/data_assim/data/USGS/Crow_2017/sites_in_ArkRed/'\n",
    "\n",
    "# --- Synthetic data --- #\n",
    "synth_openloop_data_dir = \\\n",
    "    '/civil/hydro/ymao/data_assim/tools/plot_analyze_results/' \\\n",
    "    'output/20180503.ArkRed.synthetic.LAI_from_veglib/' \\\n",
    "    'sm_0.5.prec_0.3.N32.no_sm3_update_only/synthetic/data'\n",
    "synth_data_dir = '/civil/hydro/ymao/data_assim/tools/plot_analyze_results/' \\\n",
    "                 'output/20180503.ArkRed.synthetic.LAI_from_veglib/' \\\n",
    "                 'sm_0.5.prec_0.3.N32.no_sm3_update_only/EnKF/data'\n",
    "synth_zero_updata_data_dir = \\\n",
    "    '/civil/hydro/ymao/data_assim/tools/plot_analyze_results/' \\\n",
    "    'output/20180503.ArkRed.synthetic.LAI_from_veglib/' \\\n",
    "    'sm_0.5.prec_0.3.N32.no_sm3_update_only/EnKF.zero_update/data'\n",
    "\n",
    "# --- Routed --- #\n",
    "# Openloop nc\n",
    "openloop_nc = '/civil/hydro/ymao/data_assim/output/RVIC/ArkRed.AndyWood_rout/' \\\n",
    "              'openloop.NLDAS2.2015_2017.LAI_from_veglib/hist/' \\\n",
    "              'openloop.rvic.h0a.2018-01-01-00.nc'\n",
    "# Ensemble size\n",
    "N = 32\n",
    "# Ensemble nc; \"{}\" will be replaced by ensemble index\n",
    "ensemble_basenc = '/civil/hydro/ymao/data_assim/output/RVIC/ArkRed.AndyWood_rout/' \\\n",
    "                  'ArkRed.smap.NLDAS2.qc_no_winter.LAI_from_veglib/' \\\n",
    "                  'sm_0.5.prec_0.3.N32.no_sm3_update_only/EnKF/' \\\n",
    "                  'hist/ens{}.rvic.h0a.2018-01-01-00.nc'\n",
    "\n",
    "# Time lag of routed data with local time [hours];\n",
    "# Example: if local time is UTC-6 (ArkRed) and routed data is in UTC, then time_lag = 6\n",
    "time_lag = 6\n",
    "\n",
    "# --- Zero-update --- #\n",
    "# Zero-update ensemble nc; \"{}\" will be replaced by ensemble index\n",
    "zero_update_ensemble_basenc = \\\n",
    "    '/civil/hydro/ymao/data_assim/output/RVIC/ArkRed.AndyWood_rout/' \\\n",
    "    'ArkRed.smap.NLDAS2.qc_no_winter.LAI_from_veglib/' \\\n",
    "    'sm_0.5.prec_0.3.N32.no_sm3_update_only/EnKF.zero_update/' \\\n",
    "    'hist/ens{}.rvic.h0a.2018-01-01-00.nc'\n",
    "\n",
    "# --- Time --- #\n",
    "start_time = pd.to_datetime('2015-03-31')\n",
    "end_time = pd.to_datetime('2017-12-31')\n",
    "start_year = start_time.year\n",
    "end_year = end_time.year\n",
    "\n",
    "# --- Domain file (\"mask\" and \"area\" will be used) --- #\n",
    "domain_nc = '/civil/hydro/ymao/data_assim/param/vic/ArkRed/ArkRed.domain.nc'\n",
    "\n",
    "# --- RVIC output param remap nc files --- #\n",
    "# \"{}\" will be replaced by site name\n",
    "rvic_subbasin_nc = '/civil/hydro/ymao/data_assim/param/RVIC/ArkRed/parameter/' \\\n",
    "                   'param_run_output.8sites_Crow2017/temp/remapped/remapUH_{}.nc'\n",
    "    \n",
    "# --- VIC forcing file basedir (\"YYYY.nc\" will be appended) --- #\n",
    "force_basedir = '/civil/hydro/ymao/data_assim/forcing/vic/NLDAS-2/ArkRed/force.'\n",
    "\n",
    "# --- VIC opnloop output history nc (this is to calculate baseflow fraction) --- # \n",
    "vic_openloop_hist_nc = \\\n",
    "    '/civil/hydro/ymao/data_assim/output/vic/ArkRed/LAI_from_veglib/openloop.NLDAS2.2015_2017/' \\\n",
    "    'history/history.openloop.2015-03-31-00000.nc'\n",
    "\n",
    "# --- Output --- #\n",
    "output_dir = '/civil/hydro/ymao/data_assim/tools/process_evaluation_data_ArkRed/' \\\n",
    "             'output/20180424.ArkRed.smap.LAI_from_veglib.qc_no_winter/' \\\n",
    "             'sm_0.5.prec_0.3.N32.EnKF_perturbed_force'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading USGS data for site illinois...\n",
      "Loading USGS data for site walnut...\n",
      "Loading USGS data for site deep...\n",
      "Loading USGS data for site bird...\n",
      "Loading USGS data for site chikaskia...\n",
      "Loading USGS data for site spring...\n",
      "Loading USGS data for site arkansas...\n",
      "Loading USGS data for site ninnescah...\n"
     ]
    }
   ],
   "source": [
    "# ===================================================== #\n",
    "# Load USGS data\n",
    "# ===================================================== #\n",
    "# --- Load site info --- #\n",
    "df_site_info = pd.read_csv(site_info_csv, dtype={'site_no': str})\n",
    "dict_sites = {}  # {site: site_no}\n",
    "for i in df_site_info.index:\n",
    "    site = df_site_info.loc[i, 'short_name']\n",
    "    site_no = df_site_info.loc[i, 'site_no']\n",
    "    dict_sites[site] = site_no\n",
    "    \n",
    "# --- Load USGS streamflow data --- #\n",
    "dict_flow_usgs = {}  # {site: ts}\n",
    "for site in dict_sites.keys():\n",
    "    print('Loading USGS data for site {}...'.format(site))\n",
    "    site_no = dict_sites[site]\n",
    "    filename = os.path.join(usgs_data_dir, '{}.txt'.format(site_no))\n",
    "    ts_flow = read_USGS_data(filename, columns=[1], names=['flow'])['flow']\n",
    "    dict_flow_usgs[site] = ts_flow.truncate(before=start_time, after=end_time)\n",
    "\n",
    "# --- Get USGS drainage area (mi2) --- #\n",
    "dict_usgs_drainage_area = {}  # {site: area}\n",
    "for i in df_site_info.index:\n",
    "    site = df_site_info.loc[i, 'short_name']\n",
    "    drainage_area = df_site_info.loc[i, 'drain_area_va']\n",
    "    dict_usgs_drainage_area[site] = drainage_area * 1.60934 * 1.60934  # convert [mi2] to [km2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================== #\n",
    "# Load synthetic results\n",
    "# ===================================================== #\n",
    "# --- Load PER --- #\n",
    "# Surface runoff\n",
    "da_synth_openloop_rmseLog_runoff = xr.open_dataset(\n",
    "    os.path.join(synth_openloop_data_dir, 'rmseLog_openloop_dailyRunoff.nc'))['rmse']\n",
    "da_synth_rmseLog_runoff = xr.open_dataset(\n",
    "    os.path.join(synth_data_dir, 'rmseLog_EnKF_runoffDaily.nc'))['rmse']\n",
    "da_per_synth_runoff = (1 - da_synth_rmseLog_runoff / da_synth_openloop_rmseLog_runoff) * 100\n",
    "# Baseflow\n",
    "da_synth_openloop_rmseLog_baseflow = xr.open_dataset(\n",
    "    os.path.join(synth_openloop_data_dir, 'rmseLog_openloop_dailyBaseflow.nc'))['rmse']\n",
    "da_synth_rmseLog_baseflow = xr.open_dataset(\n",
    "    os.path.join(synth_data_dir, 'rmseLog_EnKF_baseflowDaily.nc'))['rmse']\n",
    "da_per_synth_baseflow = (1 - da_synth_rmseLog_baseflow / da_synth_openloop_rmseLog_baseflow) * 100\n",
    "# Total runoff\n",
    "da_synth_openloop_rmseLog_totrunoff = xr.open_dataset(\n",
    "    os.path.join(synth_openloop_data_dir, 'rmseLog_openloop_dailyTotrunoff.nc'))['rmse']\n",
    "da_synth_rmseLog_totrunoff = xr.open_dataset(\n",
    "    os.path.join(synth_data_dir, 'rmseLog_EnKF_totrunoffDaily.nc'))['rmse']\n",
    "da_per_synth_totrunoff = (1 - da_synth_rmseLog_totrunoff / da_synth_openloop_rmseLog_totrunoff) * 100\n",
    "\n",
    "# --- Load PSR --- #\n",
    "# Surface runoff\n",
    "da_synth_crps_runoff_zero_update = xr.open_dataset(\n",
    "    os.path.join(synth_zero_updata_data_dir, 'crps_EnKF_runoff_daily_log.nc'))['crps']\n",
    "da_synth_crps_runoff = xr.open_dataset(\n",
    "    os.path.join(synth_data_dir, 'crps_EnKF_runoff_daily_log.nc'))['crps']\n",
    "da_psr_synth_runoff = (1 - da_synth_crps_runoff / da_synth_crps_runoff_zero_update) * 100\n",
    "# Baseflow\n",
    "da_synth_crps_baseflow_zero_update = xr.open_dataset(\n",
    "    os.path.join(synth_zero_updata_data_dir, 'crps_EnKF_baseflow_daily_log.nc'))['crps']\n",
    "da_synth_crps_baseflow = xr.open_dataset(\n",
    "    os.path.join(synth_data_dir, 'crps_EnKF_baseflow_daily_log.nc'))['crps']\n",
    "da_psr_synth_baseflow = (1 - da_synth_crps_baseflow / da_synth_crps_baseflow_zero_update) * 100\n",
    "# Total runoff\n",
    "da_synth_crps_totrunoff_zero_update = xr.open_dataset(\n",
    "    os.path.join(synth_zero_updata_data_dir, 'crps_EnKF_totrunoff_daily_log.nc'))['crps']\n",
    "da_synth_crps_totrunoff = xr.open_dataset(\n",
    "    os.path.join(synth_data_dir, 'crps_EnKF_totrunoff_daily_log.nc'))['crps']\n",
    "da_psr_synth_totrunoff = (1 - da_synth_crps_totrunoff / da_synth_crps_totrunoff_zero_update) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/civil/hydro/ymao/anaconda3/envs/da2/lib/python3.5/site-packages/ipykernel_launcher.py:33: FutureWarning: how in .resample() is deprecated\n",
      "the new syntax is .resample(...).mean()\n",
      "/civil/hydro/ymao/anaconda3/envs/da2/lib/python3.5/site-packages/ipykernel_launcher.py:34: DeprecationWarning: \n",
      ".resample() has been modified to defer calculations. Instead of passing 'dim' and 'how=\"mean\", instead consider using .resample(time=\"1D\").mean() \n",
      "/civil/hydro/ymao/anaconda3/envs/da2/lib/python3.5/site-packages/xarray/core/common.py:619: FutureWarning: pd.TimeGrouper is deprecated and will be removed; Please use pd.Grouper(freq=...)\n",
      "  label=label, base=base)\n",
      "/civil/hydro/ymao/anaconda3/envs/da2/lib/python3.5/site-packages/ipykernel_launcher.py:35: DeprecationWarning: \n",
      ".resample() has been modified to defer calculations. Instead of passing 'dim' and 'how=\"mean\", instead consider using .resample(time=\"1D\").mean() \n"
     ]
    }
   ],
   "source": [
    "# ===================================================== #\n",
    "# Load and process routed data\n",
    "# ===================================================== #\n",
    "# --- Load openloop data --- #\n",
    "df_openloop, dict_outlet = read_RVIC_output(openloop_nc)\n",
    "\n",
    "# --- Load ensemble data --- #\n",
    "list_da_ensemble = []\n",
    "for i in range(N):\n",
    "    filename = ensemble_basenc.format(i+1)\n",
    "    df, dict_outlet = read_RVIC_output(filename)\n",
    "    da = xr.DataArray(df, dims=['time', 'site'])\n",
    "    list_da_ensemble.append(da)\n",
    "# Concat all ensemble members\n",
    "da_ensemble = xr.concat(list_da_ensemble, dim='N')\n",
    "\n",
    "# --- Load zero-update data --- #\n",
    "list_da_ensemble = []\n",
    "for i in range(N):\n",
    "    filename = zero_update_ensemble_basenc.format(i+1)\n",
    "    df, dict_outlet = read_RVIC_output(filename)\n",
    "    da = xr.DataArray(df, dims=['time', 'site'])\n",
    "    list_da_ensemble.append(da)\n",
    "# Concat all ensemble members\n",
    "da_zero_update_ensemble = xr.concat(list_da_ensemble, dim='N')\n",
    "\n",
    "# --- Shift all routed data data to local time --- #\n",
    "df_openloop.index = df_openloop.index - pd.DateOffset(hours=time_lag)\n",
    "da_ensemble['time'] = pd.to_datetime(da_ensemble['time'].values) - pd.DateOffset(hours=time_lag)\n",
    "da_zero_update_ensemble['time'] = \\\n",
    "    pd.to_datetime(da_zero_update_ensemble['time'].values) - pd.DateOffset(hours=time_lag)\n",
    "\n",
    "# --- Average all routed data to daily (of local time) --- #\n",
    "df_openloop_daily = df_openloop.resample('1D', how='mean')\n",
    "da_ensemble_daily = da_ensemble.resample('1D', dim='time', how='mean')\n",
    "da_zero_update_ensemble_daily = da_zero_update_ensemble.resample('1D', dim='time', how='mean')\n",
    "\n",
    "# --- Calculate ensemble mean --- #\n",
    "da_ensMean_daily = da_ensemble_daily.mean(dim='N')\n",
    "da_zero_update_ensMean_daily = da_zero_update_ensemble_daily.mean(dim='N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "illinois 3276.8724308692413\n",
      "walnut 5043.581406236542\n",
      "deep 4863.3081974647885\n",
      "bird 3103.7095348407056\n",
      "chikaskia 4310.18946663719\n",
      "spring 5075.144257650728\n",
      "arkansas 3489.61244685949\n",
      "ninnescah 1983.1028074048015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/civil/hydro/ymao/anaconda3/envs/da2/lib/python3.5/site-packages/xarray/core/dtypes.py:23: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if np.issubdtype(dtype, float):\n"
     ]
    }
   ],
   "source": [
    "# ===================================================== #\n",
    "# Load basin information\n",
    "# ===================================================== #\n",
    "ds_domain = xr.open_dataset(domain_nc)\n",
    "da_area = ds_domain['area']\n",
    "da_domain = ds_domain['mask']\n",
    "\n",
    "# --- Basin domain --- #\n",
    "dict_da_frac = {}\n",
    "for site in dict_sites.keys():\n",
    "    da_frac = xr.open_dataset(rvic_subbasin_nc.format(site))['fraction']\n",
    "    dict_da_frac[site] = da_frac\n",
    "\n",
    "# --- Basin area --- #\n",
    "dict_basin_area = {}\n",
    "for site in dict_sites.keys():\n",
    "    basin_area = float(da_area.where(dict_da_frac[site]>0).sum())  # [m2]\n",
    "    basin_area = basin_area / 1000 / 1000  # convert to [km2]\n",
    "    dict_basin_area[site] = basin_area\n",
    "    print(site, basin_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/civil/hydro/ymao/anaconda3/envs/da2/lib/python3.5/site-packages/xarray/core/dtypes.py:23: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if np.issubdtype(dtype, float):\n"
     ]
    }
   ],
   "source": [
    "# ===================================================== #\n",
    "# Calculate basin baseflow fraction (average baseflow / total runoff)\n",
    "# ===================================================== #\n",
    "# --- Load openloop VIC runoff --- #\n",
    "ds_openloop = xr.open_dataset(vic_openloop_hist_nc)\n",
    "da_totrunoff = ds_openloop['OUT_RUNOFF'] + ds_openloop['OUT_BASEFLOW']\n",
    "da_baseflow = ds_openloop['OUT_BASEFLOW']\n",
    "# --- Calculate basin-avg baseflow fraction --- #\n",
    "dict_baseflow_fraction = {}\n",
    "for site in dict_sites.keys():\n",
    "    # Baseflow sum\n",
    "    da_basin_baseflow = da_baseflow.where(dict_da_frac[site]>0)\n",
    "    basin_baseflow = (da_basin_baseflow * da_area).sum(\n",
    "        dim='lat').sum(dim='lon').mean(dim='time').values  # [mm*m2/step]\n",
    "    # Total runoff sum\n",
    "    da_basin_totrunoff = da_totrunoff.where(dict_da_frac[site]>0)\n",
    "    basin_totrunoff = (da_basin_totrunoff * da_area).sum(\n",
    "        dim='lat').sum(dim='lon').mean(dim='time').values  # [mm*m2/step]\n",
    "    # Baseflow fraction\n",
    "    baseflow_fraction = basin_baseflow / basin_totrunoff\n",
    "    dict_baseflow_fraction[site] = baseflow_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/civil/hydro/ymao/anaconda3/envs/da2/lib/python3.5/site-packages/xarray/core/dtypes.py:23: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if np.issubdtype(dtype, float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "illinois \treal-data \tsynth \tsynth_surfaceRunoff \tsynth_baseflow\n",
      "PER: \t\t3.0% \t\t16.4% \t\t0.1% \t\t19.2%\n",
      "PSR: \t\t0.1% \t\t7.1% \t\t1.0% \t\t9.2%\n",
      "Baseflow fraction = 0.626\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/civil/hydro/ymao/anaconda3/envs/da2/lib/python3.5/site-packages/xarray/core/dtypes.py:23: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if np.issubdtype(dtype, float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walnut \treal-data \tsynth \tsynth_surfaceRunoff \tsynth_baseflow\n",
      "PER: \t\t7.0% \t\t15.9% \t\t2.5% \t\t20.1%\n",
      "PSR: \t\t-0.6% \t\t13.0% \t\t2.0% \t\t20.2%\n",
      "Baseflow fraction = 0.451\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/civil/hydro/ymao/anaconda3/envs/da2/lib/python3.5/site-packages/xarray/core/dtypes.py:23: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if np.issubdtype(dtype, float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep \treal-data \tsynth \tsynth_surfaceRunoff \tsynth_baseflow\n",
      "PER: \t\t0.9% \t\t9.3% \t\t0.0% \t\t13.5%\n",
      "PSR: \t\t-0.4% \t\t8.7% \t\t1.8% \t\t13.9%\n",
      "Baseflow fraction = 0.551\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/civil/hydro/ymao/anaconda3/envs/da2/lib/python3.5/site-packages/xarray/core/dtypes.py:23: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if np.issubdtype(dtype, float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird \treal-data \tsynth \tsynth_surfaceRunoff \tsynth_baseflow\n",
      "PER: \t\t1.7% \t\t11.5% \t\t0.9% \t\t17.9%\n",
      "PSR: \t\t-0.2% \t\t12.5% \t\t2.0% \t\t20.0%\n",
      "Baseflow fraction = 0.549\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/civil/hydro/ymao/anaconda3/envs/da2/lib/python3.5/site-packages/xarray/core/dtypes.py:23: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if np.issubdtype(dtype, float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chikaskia \treal-data \tsynth \tsynth_surfaceRunoff \tsynth_baseflow\n",
      "PER: \t\t2.7% \t\t10.2% \t\t4.0% \t\t22.9%\n",
      "PSR: \t\t-0.2% \t\t9.0% \t\t3.2% \t\t19.8%\n",
      "Baseflow fraction = 0.306\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/civil/hydro/ymao/anaconda3/envs/da2/lib/python3.5/site-packages/xarray/core/dtypes.py:23: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if np.issubdtype(dtype, float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spring \treal-data \tsynth \tsynth_surfaceRunoff \tsynth_baseflow\n",
      "PER: \t\t5.8% \t\t17.3% \t\t1.3% \t\t19.4%\n",
      "PSR: \t\t2.2% \t\t11.6% \t\t1.7% \t\t15.0%\n",
      "Baseflow fraction = 0.639\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/civil/hydro/ymao/anaconda3/envs/da2/lib/python3.5/site-packages/xarray/core/dtypes.py:23: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if np.issubdtype(dtype, float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arkansas \treal-data \tsynth \tsynth_surfaceRunoff \tsynth_baseflow\n",
      "PER: \t\t3.9% \t\t50.7% \t\t3.2% \t\t71.7%\n",
      "PSR: \t\t-0.4% \t\t8.4% \t\t2.9% \t\t24.6%\n",
      "Baseflow fraction = 0.282\n",
      "\n",
      "\n",
      "ninnescah \treal-data \tsynth \tsynth_surfaceRunoff \tsynth_baseflow\n",
      "PER: \t\t-2.0% \t\t8.6% \t\t3.7% \t\t39.5%\n",
      "PSR: \t\t0.1% \t\t8.3% \t\t4.6% \t\t18.8%\n",
      "Baseflow fraction = 0.320\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/civil/hydro/ymao/anaconda3/envs/da2/lib/python3.5/site-packages/xarray/core/dtypes.py:23: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if np.issubdtype(dtype, float):\n"
     ]
    }
   ],
   "source": [
    "# ===================================================== #\n",
    "# Calculate and save subbasin metrics for both synthetic and real-data cases\n",
    "# ===================================================== #\n",
    "for site in dict_sites.keys():\n",
    "    # --- Some calculation --- #\n",
    "    ts_usgs = dict_flow_usgs[site]\n",
    "    ts_openloop = df_openloop_daily.loc[:, site]\n",
    "    ts_usgs = ts_usgs[ts_openloop.index].dropna()\n",
    "    ts_openloop = ts_openloop[ts_usgs.index].dropna()\n",
    "    ts_ensMean = da_ensMean_daily.sel(site=site).to_series()[ts_usgs.index].dropna()\n",
    "    ensemble_daily = da_ensemble_daily.sel(\n",
    "        site=site, time=ts_usgs.index).transpose('time', 'N').values\n",
    "    zero_update_ensemble_daily = da_zero_update_ensemble_daily.sel(\n",
    "        site=site, time=ts_usgs.index).transpose('time', 'N').values\n",
    "    \n",
    "    # --- Real-data routed streamflow --- #\n",
    "    # PER\n",
    "    rmseLog_openloop = rmse(np.log(ts_openloop+1), np.log(ts_usgs+1))\n",
    "    rmseLog_ensMean = rmse(np.log(ts_ensMean+1), np.log(ts_usgs+1))\n",
    "    per = (1 - rmseLog_ensMean / rmseLog_openloop) * 100\n",
    "    # PSR\n",
    "    crps_ens = crps(ts_usgs, ensemble_daily)\n",
    "    crps_zero_update = crps(ts_usgs, zero_update_ensemble_daily)\n",
    "    psr = (1 - crps_ens / crps_zero_update) * 100\n",
    "    # Normalized ensemble skill (NENSK)\n",
    "    nensk_ens = nensk(ts_usgs, ensemble_daily)\n",
    "\n",
    "    # --- Synthetic, subbasin median --- #\n",
    "    # PER\n",
    "    per_synth_totrunoff = da_per_synth_totrunoff.where(dict_da_frac[site]>0).median().values\n",
    "    per_synth_surfacerunoff = da_per_synth_runoff.where(dict_da_frac[site]>0).median().values\n",
    "    per_synth_baseflow = da_per_synth_baseflow.where(dict_da_frac[site]>0).median().values\n",
    "    # PSR\n",
    "    psr_synth_totrunoff = da_psr_synth_totrunoff.where(dict_da_frac[site]>0).median().values\n",
    "    psr_synth_surfacerunoff = da_psr_synth_runoff.where(dict_da_frac[site]>0).median().values\n",
    "    psr_synth_baseflow = da_psr_synth_baseflow.where(dict_da_frac[site]>0).median().values\n",
    "    \n",
    "    # --- Print and save results --- #\n",
    "    print('{} \\treal-data \\tsynth \\tsynth_surfaceRunoff \\tsynth_baseflow'.format(\n",
    "        site))\n",
    "    print('PER: \\t\\t{:.1f}% \\t\\t{:.1f}% \\t\\t{:.1f}% \\t\\t{:.1f}%'.format(\n",
    "        per, per_synth_totrunoff, per_synth_surfacerunoff, per_synth_baseflow))\n",
    "    print('PSR: \\t\\t{:.1f}% \\t\\t{:.1f}% \\t\\t{:.1f}% \\t\\t{:.1f}%'.format(\n",
    "        psr, psr_synth_totrunoff, psr_synth_surfacerunoff, psr_synth_baseflow))\n",
    "    print('Baseflow fraction = {:.3f}'.format(dict_baseflow_fraction[site]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_RVIC_output(filepath, output_format='array', outlet_ind=-1):\n",
    "    ''' This function reads RVIC output netCDF file\n",
    "\n",
    "    Input:\n",
    "        filepath: path of the output netCDF file\n",
    "        output_format: 'array' or 'grid' (currently only support 'array')\n",
    "        outlet_ind: index of the outlet to be read (index starts from 0); -1 for reading all outlets\n",
    "\n",
    "    Return:\n",
    "        df - a DataFrame containing streamflow [unit: cfs]; column name(s): outlet name\n",
    "        dict_outlet - a dictionary with outlet name as keys; [lat lon] as content\n",
    "\n",
    "    '''\n",
    "    \n",
    "    ds = xr.open_dataset(filepath)\n",
    "\n",
    "    #=== Read in outlet names ===#\n",
    "    outlet_names = [outlet_name.decode('utf-8')\n",
    "                    for outlet_name in ds['outlet_name'].values]\n",
    "\n",
    "    #=== Read in outlet lat lon ===#\n",
    "    dict_outlet = {}\n",
    "    # If read all outlets\n",
    "    if outlet_ind==-1:\n",
    "        for i, name in enumerate(outlet_names):\n",
    "            dict_outlet[name] = [ds['lat'].values[i], ds['lon'].values[i]]\n",
    "    # If read one outlet\n",
    "    else:\n",
    "        dict_outlet[outlet_names[outlet_ind]] = \\\n",
    "                        [ds['lat'].values[outlet_ind], ds['lon'].values[outlet_ind]]\n",
    "\n",
    "    #=== Read in streamflow variable ===#\n",
    "    flow = ds['streamflow'].values\n",
    "    flow = flow * np.power(1000/25.4/12, 3)  # convert m3/s to cfs\n",
    "    # If read all outlets\n",
    "    if outlet_ind==-1:\n",
    "        df = pd.DataFrame(flow, index=ds.coords['time'].values, columns=outlet_names)\n",
    "    # If read one outlet\n",
    "    else:\n",
    "        df = pd.DataFrame(flow[:,outlet_ind], index=ds.coords['time'].values, \\\n",
    "                          columns=[outlet_names[outlet_ind]])\n",
    "\n",
    "    return df, dict_outlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_USGS_data(file, columns, names):\n",
    "    '''This function reads USGS streamflow from the directly downloaded format (date are in the 3rd columns)\n",
    "\n",
    "    Input:\n",
    "        file: directly downloaded streamflow file path [str]\n",
    "        columns: a list of data colomn numbers, starting from 1.\n",
    "            E.g., if the USGS original data has three variables: max_flow, min_flow,\n",
    "            mean_flow, and the desired variable is mean_flow, then columns = [3]\n",
    "        names: a list of data column names. E.g., ['mean_flow']; must the same length as columns\n",
    "\n",
    "    Return:\n",
    "        a pd.DataFrame object with time as index and data columns (NaN for missing data points)\n",
    "\n",
    "    Note: returned data and flow might not be continuous if there is missing data!!!\n",
    "\n",
    "    '''\n",
    "    ndata = len(columns)\n",
    "    if ndata != len(names):  # check input validity\n",
    "        raise ValueError(\"Input arguments 'columns' and 'names' must have same length!\")\n",
    "\n",
    "    f = open(file, 'r')\n",
    "    date_array = []\n",
    "    data = []\n",
    "    for i in range(ndata):\n",
    "        data.append([])\n",
    "    while 1:\n",
    "        line = f.readline().rstrip(\"\\n\")  # read in one line\n",
    "        if line==\"\":\n",
    "                break\n",
    "        line_split = line.split('\\t')\n",
    "        if line_split[0]=='USGS':  # if data line\n",
    "                date_string = line_split[2]  # read in date string\n",
    "                date = dt.datetime.strptime(date_string, \"%Y-%m-%d\")  # convert date to dt object\n",
    "                date_array.append(date)\n",
    "\n",
    "                for i in range(ndata):  # for each desired data variable\n",
    "                        col = columns[i]\n",
    "                        if line_split[3+(col-1)*2] == '':  # if data is missing\n",
    "                                value = np.nan\n",
    "                        elif line_split[3+(col-1)*2] == 'Ice':  # if data is 'Ice'\n",
    "                                value = np.nan\n",
    "                        else:  # if data is not missing\n",
    "                                value = float(line_split[3+(col-1)*2])\n",
    "                        data[i].append(value)\n",
    "\n",
    "    data = np.asarray(data).transpose()\n",
    "    df = pd.DataFrame(data, index=date_array, columns=names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kge(sim, obs):\n",
    "    ''' Calculate Kling-Gupta Efficiency (function from Oriana) '''\n",
    "\n",
    "    std_sim = np.std(sim)\n",
    "    std_obs = np.std(obs)\n",
    "    mean_sim = sim.mean(axis=0)\n",
    "    mean_obs = obs.mean(axis=0)\n",
    "    r_array = np.corrcoef(sim.values, obs.values)\n",
    "    r = r_array[0,1]\n",
    "    relvar = std_sim/std_obs\n",
    "    bias = mean_sim/mean_obs\n",
    "    kge = 1-np.sqrt(np.square(r-1) + np.square(relvar-1) + np.square(bias-1))\n",
    "    return kge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nse(sim, obs):\n",
    "    ''' Calcualte Nash–Sutcliffe efficiency'''\n",
    "    \n",
    "    obs_mean = np.mean(obs)\n",
    "    nse = 1 - np.sum(np.square(sim - obs)) / np.sum(np.square(obs - obs_mean))\n",
    "    return nse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(true, est):\n",
    "    ''' Calculates RMSE of an estimated variable compared to the truth variable\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    true: <np.array>\n",
    "        A 1-D array of time series of true values\n",
    "    est: <np.array>\n",
    "        A 1-D array of time series of estimated values (must be the same length of true)\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    rmse: <float>\n",
    "        Root mean square error\n",
    "\n",
    "    Require\n",
    "    ----------\n",
    "    numpy\n",
    "    '''\n",
    "\n",
    "    rmse = np.sqrt(sum((est - true)**2) / len(true))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crps(truth, ensemble):\n",
    "    ''' Calculate mean CRPS of an ensemble time series\n",
    "    Parameters\n",
    "    ----------\n",
    "    truth: <np.array>\n",
    "        A 1-D array of truth time series\n",
    "        Dimension: [n]\n",
    "    ensemble: <np.array>\n",
    "        A 2-D array of ensemble time series\n",
    "        Dimension: [n, N], where N is ensemble size; n is time series length\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    crps: <float>\n",
    "        Time-series-mean CRPS\n",
    "        \n",
    "    Require\n",
    "    ----------\n",
    "    import properscoring as ps\n",
    "    '''\n",
    "    \n",
    "    array_crps = np.asarray([ps.crps_ensemble(truth[t], ensemble[t, :]) for t in range(len(truth))])\n",
    "    crps = array_crps.mean()\n",
    "    \n",
    "    return crps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_ensemble_norm_var(truth, ensemble):\n",
    "    ''' Calculate variance of normalized bias of an ensemble time series.\n",
    "    Specifically, at each time step t, mean bias is normalized by ensemble spread:\n",
    "            bias_norm(t) = mean_bias / std(ensemble)\n",
    "    Then average over all time steps:\n",
    "            bias_norm = mean(bias_norm(t))\n",
    "            \n",
    "    Parameters\n",
    "    ----------\n",
    "    truth: <np.array>\n",
    "        A 1-D array of truth time series\n",
    "        Dimension: [n]\n",
    "    ensemble: <np.array>\n",
    "        A 2-D array of ensemble time series\n",
    "        Dimension: [n, N], where N is ensemble size; n is time series length\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    bias_ensemble_norm_var: <float>\n",
    "        Time-series-mean ensemble-normalized bias\n",
    "        \n",
    "    Require\n",
    "    ----------\n",
    "    import properscoring as ps\n",
    "    '''\n",
    "    \n",
    "    mean_bias = ensemble.mean(axis=1) - truth  # [n]\n",
    "    std_ensemble = ensemble.std(axis=1)  # [n]\n",
    "    bias_ensemble_norm_var = (mean_bias / std_ensemble).var()\n",
    "    \n",
    "    return bias_ensemble_norm_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nensk(truth, ensemble):\n",
    "    ''' Calculate the ratio of temporal-mean ensemble skill to temporal-mean ensemble spread:\n",
    "            nensk = <ensk> / <ensp>\n",
    "    where <ensk> is temporal average of: ensk(t) = (ensmean - truth)^2\n",
    "          <ensp> is temperal average of: ensp(t) = mean((ens_i - ensmean)^2) = var(ens_i)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    truth: <np.array>\n",
    "        A 1-D array of truth time series\n",
    "        Dimension: [n]\n",
    "    ensemble: <np.array>\n",
    "        A 2-D array of ensemble time series\n",
    "        Dimension: [n, N], where N is ensemble size; n is time series length\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    nensk: <float>\n",
    "        Normalized ensemble skill\n",
    "    '''\n",
    "\n",
    "    ensk = np.square((ensemble.mean(axis=1) - truth))  # [n]\n",
    "    ensp = ensemble.var(axis=1)  # [n]\n",
    "    nensk = np.mean(ensk) / np.mean(ensp)\n",
    "\n",
    "    return nensk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
